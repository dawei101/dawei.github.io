---
id: 71
title: 超大型网站架构设计
date: 2011-03-30T18:37:43+00:00
author: dawei
layout: post
permalink: /2011/03/30/%e8%b6%85%e5%a4%a7%e5%9e%8b%e7%bd%91%e7%ab%99%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1/
categories:
  - Tech
---

这是我在网上无意中看到的一篇文章,介绍了myspace的六次重构,对于做海量用户系统的朋友来说,应该能从中受到很多启发.  
在每个里程碑，站点负担都会超过底层系统部分组件的最大载荷，特别是数据库和存储系统。接着，功能出现问题，用户失声尖叫。最后，技术团队必须为此修订系统策略。  
虽然自2005年早期，站点账户数超过7百万后，系统架构到目前为止保持了相对稳定，但MySpace仍然在为SQL Server支持的同时连接数等方面继续攻坚，Benedetto说，"我们已经尽可能把事情做到最好"。  

### 里程碑一：50万账户  

按Benedetto 的说法，MySpace最初的系统很小，只有两台Web服务器和一个数据库服务器。那时使用的是Dell双CPU、4G内存的系统。  

单个数据库就意味着所有数据都存储在一个地方，再由两台Web服务器分担处理用户请求的工作量。但就像MySpace后来的几次底层系统修订时的情况一样，三服务器架构很快不堪重负。此后一个时期内，MySpace基本是通过添置更多Web服务器来对付用户暴增问题的。  

但到在2004年早期，MySpace用户数增长到50万后，数据库服务器也已开始汗流浃背。  

但和Web服务器不同，增加数据库可没那么简单。如果一个站点由多个数据库支持，设计者必须考虑的是，如何在保证数据一致性的前提下，让多个数据库分担压力。  

在第二代架构中，MySpace运行在3个SQL Server数据库服务器上——一个为主，所有的新数据都向它提交，然后由它复制到其他两个；另两个全力向用户供给数据，用以在博客和个人资料栏显示。这 种方式在一段时间内效果很好——只要增加数据库服务器，加大硬盘，就可以应对用户数和访问量的增加。  

### 里程碑二：1-2百万账户  

MySpace注册数到达1百万至2百万区间后，数据库服务器开始受制于I/O容量——即它 们存取数据的速度。而当时才是2004年中，距离上次数据库系统调整不过数月。用户的提交请求被阻塞，就像千人乐迷要挤进只能容纳几百人的夜总会，站点开 始遭遇"主要矛盾"，Benedetto说，这意味着 MySpace永远都会轻度落后于用户需求。  

"有人花5分钟都无法完成留言，因此用户总是抱怨说网站已经完蛋了。"他补充道。  

这一次的数据库架构按照垂直分割模式设计，不同的数据库服务于站点的不同功能，如登录、用户资料和博客。于是，站点的扩展性问题看似又可以告一段落了，可以歇一阵子。  

垂直分割策略利于多个数据库分担访问压力，当用户要求增加新功能时，MySpace将投入新 的数据库予以支持它。账户到达2百万后，MySpace还从存储设备与数据库服务器直接交互的方式切换到SAN（Storage Area Network，存储区域网络）——用高带宽、专门设计的网络将大量磁盘存储设备连接在一起，而数据库连接到SAN。这项措施极大提升了系统性能、正常运 行时间和可靠性，Benedetto说。  

### 里程碑三：3百万账户  

当用户继续增加到3百万后，垂直分割策略也开始难以为继。尽管站点的各个应用被设计得高度独 立，但有些信息必须共享。在这个架构里，每个数据库必须有各自的用户表副本——MySpace授权用户的电子花名册。这就意味着一个用户注册时，该条账户 记录必须在9个不同数据库上分别创建。但在个别情况下，如果其中某台数据库服务器临时不可到达，对应事务就会失败，从而造成账户非完全创建，最终导致此用 户的该项服务无效。  

另外一个问题是，个别应用如博客增长太快，那么专门为它服务的数据库就有巨大压力。  

2004年中，MySpace面临Web开发者称之为"向上扩展"对"向外扩展"（译者 注：Scale Up和Scale Out，也称硬件扩展和软件扩展）的抉择——要么扩展到更大更强、也更昂贵的服务器上，要么部署大量相对便宜的服务器来分担数据库压力。一般来说，大型站 点倾向于向外扩展，因为这将让它们得以保留通过增加服务器以提升系统能力的后路。  

但成功地向外扩展架构必须解决复杂的分布式计算问题，大型站点如Google、Yahoo和Amazon.com，都必须自行研发大量相关技术。以Google为例，它构建了自己的分布式文件系统。  

另外，向外扩展策略还需要大量重写原来软件，以保证系统能在分布式服务器上运行。"搞不好，开发人员的所有工作都将白费"，Benedetto说。  

因此，MySpace首先将重点放在了向上扩展上，花费了大约1个半月时间研究升级到32CPU服务器以管理更大数据库的问题。Benedetto说，"那时候，这个方案看似可能解决一切问题。"如稳定性，更棒的是对现有软件几乎没有改动要求。  

糟糕的是，高端服务器极其昂贵，是购置同样处理能力和内存速度的多台服务器总和的很多倍。而且，站点架构师预测，从长期来看，即便是巨型数据库，最后也会不堪重负，Benedetto说，"换句话讲，只要增长趋势存在，我们最后无论如何都要走上向外扩展的道路。"  

因此，MySpace最终将目光移到分布式计算架构——它在物理上分布的众多服务器，整体必 须逻辑上等同于单台机器。拿数据库来说，就不能再像过去那样将应用拆分，再以不同数据库分别支持，而必须将整个站点看作一个应用。现在，数据库模型里只有 一个用户表，支持博客、个人资料和其他核心功能的数据都存储在相同数据库。  

既然所有的核心数据逻辑上都组织到一个数据库，那么MySpace必须找到新的办法以分担负 荷——显然，运行在普通硬件上的单个数据库服务器是无能为力的。这次，不再按站点功能和应用分割数据库，MySpace开始将它的用户按每百万一组分割， 然后将各组的全部数据分别存入独立的SQL Server实例。目前，MySpace的每台数据库服务器实际运行两个SQL Server实例，也就是说每台服务器服务大约2百万用户。Benedetto指出，以后还可以按照这种模式以更小粒度划分架构，从而优化负荷分担。  

当然，还是有一个特殊数据库保存了所有账户的名称和密码。用户登录后，保存了他们其他数据的数据库再接管服务。特殊数据库的用户表虽然庞大，但它只负责用户登录，功能单一，所以负荷还是比较容易控制的。  

### 里程碑四：9百万到1千7百万账户  
2005年早期，账户达到9百万后，MySpace开始用Microsoft的C#编写 ASP.NET程序。C#是C语言的最新派生语言，吸收了C++和Java的优点，依托于Microsoft .NET框架（Microsoft为软件组件化和分布式计算而设计的模型架构）。ASP.NET则由编写Web站点脚本的ASP技术演化而来，是 Microsoft目前主推的Web站点编程环境。  

可以说是立竿见影， MySpace马上就发现ASP.NET程序运行更有效率，与ColdFusion相比，完成同样任务需消耗的处理器能力更小。据技术总监 Whitcomb说，新代码需要150台服务器完成的工作，如果用ColdFusion则需要246台。Benedetto还指出，性能上升的另一个原因 可能是在变换软件平台，并用新语言重写代码的过程中，程序员复审并优化了一些功能流程。  

最终，MySpace开始大规模迁移到ASP.NET。即便剩余的少部分 ColdFusion代码，也从Cold-Fusion服务器搬到了 ASP.NET，因为他们得到了BlueDragon.NET（乔治亚州阿尔法利塔New Atlanta Communications公司的产品，它能将ColdFusion代码自动重新编译到Microsoft平台）的帮助。  

账户达到1千万时，MySpace再次遭遇存储瓶颈问题。SAN的引入解决了早期一些性能问题，但站点目前的要求已经开始周期性超越SAN的I/O容量——即它从磁盘存储系统读写数据的极限速度。  

原因之一是每数据库1百万账户的分割策略，通常情况下的确可以将压力均分到各台服务器，但现实并非一成不变。比如第七台账户数据库上线后，仅仅7天就被塞满了，主要原因是佛罗里达一个乐队的歌迷疯狂注册。  

某个数据库可能因为任何原因，在任何时候遭遇主要负荷，这时，SAN中绑定到该数据库的磁盘存储设备簇就可能过载。"SAN让磁盘I/O能力大幅提升了，但将它们绑定到特定数据库的做法是错误的。"Benedetto说。  

最初，MySpace通过定期重新分配SAN中数据，以让其更为均衡的方法基本解决了这个问题，但这是一个人工过程，"大概需要两个人全职工作。"Benedetto说。  

长期解决方案是迁移到虚拟存储体系上，这样，整个SAN被当作一个巨型存储池，不再要求每个磁盘为特定应用服务。MySpace目前采用了一种新型SAN设备——来自加利福尼亚州弗里蒙特的3PARdata。  

在3PAR的系统里，仍能在逻辑上按容量划分数据存储，但它不再被绑定到特定磁盘或磁盘簇， 而是散布于大量磁盘。这就使均分数据访问负荷成为可能。当数据库需要写入一组数据时，任何空闲磁盘都可以马上完成这项工作，而不再像以前那样阻塞在可能已 经过载的磁盘阵列处。而且，因为多个磁盘都有数据副本，读取数据时，也不会使SAN的任何组件过载。  

当2005年春天账户数达到1千7百万时，MySpace又启用了新的策略以减轻存储系统压 力，即增加数据缓存层——位于Web服务器和数据库服务器之间，其唯一职能是在内存中建立被频繁请求数据对象的副本，如此一来，不访问数据库也可以向 Web应用供给数据。换句话说，100个用户请求同一份资料，以前需要查询数据库100次，而现在只需1次，其余都可从缓存数据中获得。当然如果页面变 化，缓存的数据必须从内存擦除，然后重新从数据库获取——但在此之前，数据库的压力已经大大减轻，整个站点的性能得到提升。  

缓存区还为那些不需要记入数据库的数据提供了驿站，比如为跟踪用户会话而创建的临时文件——Benedetto坦言他需要在这方面补课，"我是数据库存储狂热分子，因此我总是想着将万事万物都存到数据库。"但将像会话跟踪这类的数据也存到数据库，站点将陷入泥沼。  

增加缓存服务器是"一开始就应该做的事情，但我们成长太快，以致于没有时间坐下来好好研究这件事情。"Benedetto补充道。  

### 里程碑五：2千6百万账户  

2005年中期，服务账户数达到2千6百万时，MySpace切换到了还处于beta测试的 SQL Server 2005。转换何太急？主流看法是2005版支持64位处理器。但Benedetto说，"这不是主要原因，尽管这也很重要；主要还是因为我们对内存的渴 求。"支持64位的数据库可以管理更多内存。  

更多内存就意味着更高的性能和更大的容量。原来运行32位版本的SQL Server服务器，能同时使用的内存最多只有4G。切换到64位，就好像加粗了输水管的直径。升级到SQL Server 2005和64位Windows Server 2003后，MySpace每台服务器配备了32G内存，后于2006年再次将配置标准提升到64G。  

### 意外错误  

如果没有对系统架构的历次修改与升级，MySpace根本不可能走到今天。但是，为什么系统还经常吃撑着了？很多用户抱怨的"意外错误"是怎么引起的呢？  

原因之一是MySpace对Microsoft的Web技术的应用已经进入连 Microsoft自己也才刚刚开始探索的领域。比如11月，超出SQL Server最大同时连接数，MySpace系统崩溃。Benedetto说，这类可能引发系统崩溃的情况大概三天才会出现一次，但仍然过于频繁了，以致 惹人恼怒。一旦数据库罢工，"无论这种情况什么时候发生，未缓存的数据都不能从SQL Server获得，那么你就必然看到一个&#8217;意外错误&#8217;提示。"他解释说。  

去年夏天，MySpace的Windows 2003多次自动停止服务。后来发现是操作系统一个内置功能惹的祸——预防分布式拒绝服务攻击（黑客使用很多客户机向服务器发起大量连接请求，以致服务器 瘫痪）。MySpace和其他很多顶级大站点一样，肯定会经常遭受攻击，但它应该从网络级而不是依靠Windows本身的功能来解决问题——否则，大量 MySpace合法用户连接时也会引起服务器反击。  

"我们花了大约一个月时间寻找Windows 2003服务器自动停止的原因。"Benedetto说。最后，通过Microsoft的帮助，他们才知道该怎么通知服务器："别开枪，是友军。"  

紧接着是在去年7月某个周日晚上，MySpace总部所在地洛杉矶停电，造成整个系统停运 12小时。大型Web站点通常要在地理上分布配置多个数据中心以预防单点故障。本来，MySpace还有其他两个数据中心以应对突发事件，但Web服务器 都依赖于部署在洛杉矶的SAN。没有洛杉矶的SAN，Web服务器除了恳求你耐心等待，不能提供任何服务。  

Benedetto说，主数据中心的可靠性通过下列措施保证：可接入两张不同电网，另有后备电源和一台储备有30天燃料的发电机。但在这次事故中，不仅两张电网失效，而且在切换到备份电源的过程中，操作员烧掉了主动力线路。  

2007年中，MySpace在另两个后备站点上也建设了SAN。这对分担负荷大有帮助——正常情况下，每个SAN都能负担三分之一的数据访问量。而在紧急情况下，任何一个站点都可以独立支撑整个服务，Benedetto说。  

MySpace仍然在为提高稳定性奋斗，虽然很多用户表示了足够信任且能原谅偶现的错误页面。  

"作为开发人员，我憎恶Bug，它太气人了。"Dan Tanner这个31岁的德克萨斯软件工程师说，他通过MySpace重新联系到了高中和大学同学。"不过，MySpace对我们的用处很大，因此我们可 以原谅偶发的故障和错误。" Tanner说，如果站点某天出现故障甚至崩溃，恢复以后他还是会继续使用。  

这就是为什么Drew在论坛里咆哮时，大部分用户都告诉他应该保持平静，如果等几分钟，问题 就会解决的原因。Drew无法平静，他写道，"我已经两次给 MySpace发邮件，而它说一小时前还是正常的，现在出了点问题……完全是一堆废话。"另一个用户回复说，"毕竟它是免费的。"Benedetto坦承 100%的可靠性不是他的目标。"它不是银行，而是一个免费的服务。"他说。  

换句话说，MySpace的偶发故障可能造成某人最后更新的个人资料丢失，但并不意味着网站 弄丢了用户的钱财。"关键是要认识到，与保证站点性能相比，丢失少许数据的故障是可接受的。"Benedetto说。所以，MySpace甘冒丢失2分钟 到2小时内任意点数据的危险，在SQL Server配置里延长了"checkpoint"操作——它将待更新数据永久记录到磁盘——的间隔时间，因为这样做可以加快数据库的运行。  

Benedetto说，同样，开发人员还经常在几个小时内就完成构思、编码、测试和发布全过 程。这有引入Bug的风险，但这样做可以更快实现新功能。而且，因为进行大规模真实测试不具可行性，他们的测试通常是在仅以部分活跃用户为对象，且用户对 软件新功能和改进不知就里的情况下进行的。因为事实上不可能做真实的加载测试，他们做的测试通常都是针对站点。  

"我们犯过大量错误，"Benedetto说，"但到头来，我认为我们做对的还是比做错的多。"  

MySpace Tech Roster  

January 16, 2007  
By David F. Carr  


## 我的一些思考:  
看完这篇文章首先很惊讶myspace居然是基于.net平台的系 统,windows2003+asp.net+IIS+SQLSERVER,虽然我是一个纯粹的MS技术者,但是对于MS的WEB应用总是有点疑虑,我的 担心大概是来自于到处都在鼓吹的LAMP,我并不敌视开源,但是我承认,它给我带来了很大的压力.看了这篇文章,我有点庆幸也有点笑自己之前的迂腐,其实 对于技术来说,优劣只在于开发者本身,myspace的六次重构,很好的说明了这一点.  

我们常常讲分布式系统,这个概念很大,简单点说,就是把系统给拆分开来,把本来一台服务器做 的事,分到两台服务器来做.如果只在一台服务器上部署两个程序来执行之前一个程序做的事,我认为那不叫分布式,也是完全没必要的,分布式的原则是"后分 布"(lazy distribute),因为这本身是件损耗性能的工作,如果不能通过它来获得明显的性能提升,那为什么还要分布式?  

myspace的六次重构基本都是围绕数据库来进行的,做了这么多年的WEB应用,我早就知 道了数据库对于一个网站来说是多么的重要.在程序没有额外的问题的基础上,随着流量的增大,最先出现问题的,往往就是数据库.症状通常为,数据库查询/ 更新变的非常非常的慢,经常超时等.  

这个时候,我们能做的事非常有限,无非是调整下SQLServer的内存大小,给服务器加内 存换速度更快的硬盘等.如果要认真的解决这个问题,对于一般的开发者来说,都会自然的想到拆库.海量的数据查询对于SQL来说本就是不该存在的,无论你的 数据库优化的多么好,比如你用的Oracle,数据吞吐量很大,难道查询100W的记录会比SQL查询1W条更快吗?拆库又分本地数据库分多个表和多个服 务器分多个库,执行顺序为由前到後,当数据库的压力由海量查询变成磁盘IO之后,就需要更多的数据库服务器了.  

除了硬件手段,更常用的是采用缓存.myspace直到第四次改版的后期才开始正式的引入缓 存策略,这确实是个失误,如果从最开始就考虑到缓存的作用的话,那么数据库服务器至少不会膨胀的那么快,也可以有更多的时间来思考更好的架构.当然,正如 Benedetto所说,这是件一开始就该考虑的事,但是 myspace成长的太快了.  

不是谁都有机会来为一个流量在世界上排名前10的网站来设计架构,因此myspace的后面 的若干次重构,对于普通的开发者已经很难接触了,我认为,在第三个里程碑时,也就是通过服务器的横向扩展来实现的分布式,已经足够支持巨型网站的应用 了.myspace是个交互型特别强的网站,用户执行的请求数会远大于一般的网站,这也是它巨大的数据库压力的来源.  

最后总结一下WEB系统分布式的要点:  
1. 不到需要,绝对不要分布  
2. 分布式应该围绕数据库展开  
3. 分布式系统本身具有很强的扩展性,系统性能的提升和硬件的增加是线性关系.
